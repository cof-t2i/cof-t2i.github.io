<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="description" content="CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CoF-T2I</title>

  <link rel="icon" href="static/images/logo.png" type="image/png">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans:400,500,700|Noto+Sans:400,500" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css">

  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      },
      options: {
        ignoreHtmlClass: 'tex2jax_ignore',
        processHtmlClass: 'tex2jax_process'
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <style>
    :root {
      --color-primary: #F6B36A;
      --color-primary-dark: #E67E22;
      --color-text: #2c3e50;
      --color-bg: #FAFAFA;
      
      --color-think: #F3E060;
      --color-gen: #4FB6B8;
      --color-ref: #9C7BD8;
        
      --color-train:   #F1C7A1;  /* soft peach */
      --color-infer:   #B7CDEB;  /* light steel blue */
      --color-quality: #BFD8C2;  /* soft sage green */
    }

    html { scroll-behavior: smooth; }

    body {
      font-family: 'Google Sans', sans-serif;
      color: var(--color-text);
      background-color: var(--color-bg);
      line-height: 1.6;
      margin: 0;
    }

    .container {
      max-width: 1000px;
      margin: 0 auto;
      padding: 40px 20px;
    }

    /* --- Header Layout Fixed --- */
    .header-container { 
      text-align: center; 
      margin-bottom: 40px; 
    }

    .title {
      font-size: 2.5rem;
      line-height: 1.2;
      margin: 0 0 1.5rem 0;
      font-weight: 800;
    }

    /* Flex Container for Logo + Main Title */
    .main-title-row {
      display: flex;            /* Enable Flexbox */
      align-items: center;      /* Vertically center */
      justify-content: center;  /* Horizontally center */
      gap: 15px;                /* Space between logo and text */
      margin-bottom: 5px;
    }

    .project-logo {
      /* Removed absolute positioning that was hiding the image */
      width: auto;      /* Width determined by height */
      height: 1.5em;    /* Match text size approx */
      object-fit: contain;
    }
    
    .title-main { 
      color: #E67E22; 
      font-style: italic; 
    }
    .title-sub { 
      color: #000000; 
      font-size: 0.75em; 
      display: block; 
      margin-top: 8px; 
    }

    .author-block { font-size: 1.15rem; margin-bottom: 8px; }
    .author-block a { color: #222; text-decoration: none; border-bottom: 1px dotted #888; transition: 0.2s; }
    .author-block a:hover { color: var(--color-primary-dark); }
    
    .affiliation-block { font-size: 1rem; color: #666; margin-bottom: 2rem; }
    .note { font-size: 0.85rem; color: #999; }

    /* Buttons */
    .publication-links {
      display: flex;
      justify-content: center;
      gap: 12px;
      flex-wrap: wrap;
    }
    .link-block a {
      display: inline-flex;
      align-items: center;
      background: #363636;
      color: #fff;
      padding: 10px 24px;
      border-radius: 99px;
      text-decoration: none;
      font-size: 1rem;
      font-weight: 500;
      transition: all 0.3s ease;
      box-shadow: 0 2px 5px rgba(0,0,0,0.1);
    }
    .link-block a:hover {
      background: var(--color-primary);
      transform: translateY(-2px);
      box-shadow: 0 5px 10px rgba(0,0,0,0.15);
    }
    .icon { margin-right: 8px; }

    /* Sections */
    .section {
      background: #fff;
      padding: 35px;
      border-radius: 16px;
      box-shadow: 0 4px 20px rgba(0,0,0,0.03);
      margin-bottom: 35px;
    }
    .section-title {
      text-align: center;
      font-size: 1.8rem;
      margin-bottom: 25px;
      position: relative;
      font-weight: 700;
      color: #444;
    }
    .section-title::after {
      content: ''; display: block; width: 50px; height: 4px;
      background: var(--color-primary); margin: 12px auto 0; border-radius: 2px;
    }

    .content-text {
      font-size: 1.1rem;
      text-align: justify;
      color: #4a4a4a;
    }
    .highlight-term { color: var(--color-primary-dark); font-weight: 700; }

    /* Images */
    .image-container { text-align: center; margin: 10px 0; }
    .image-container img {
      max-width: 100%;
      border-radius: 8px;
      box-shadow: 0 5px 15px rgba(0,0,0,0.05);
    }

    .teaser-img { width: 95%; } 

    .caption {
      font-size: 0.95rem;
      color: #666;
      margin-top: 15px;
      text-align: center;
      font-style: italic;
      max-width: 90%;
      margin: 15px auto 0 auto;
    }

    /* --- Method Grid --- */
    .method-grid {
      display: grid;
      grid-template-columns: repeat(3, 1fr); 
      gap: 20px;
      margin-top: 30px;
    }

    .method-card {
      background: #fff;
      border: 1px solid #eee;
      padding: 25px;
      border-radius: 12px;
      transition: transform 0.2s;
      display: flex;
      flex-direction: column;
      min-width: 0; 
    }
    .method-card:hover {
      transform: translateY(-3px);
      box-shadow: 0 10px 25px rgba(0,0,0,0.08);
    }
    
    /* .method-think { border-top: 5px solid var(--color-think); }
    .method-gen   { border-top: 5px solid var(--color-gen); }
    .method-ref   { border-top: 5px solid var(--color-ref); } */
    .method-think { border-top: 5px solid var(--color-train); }
    .method-gen   { border-top: 5px solid var(--color-infer); }
    .method-ref   { border-top: 5px solid var(--color-quality); }

    .method-card h3 { 
      margin-top: 0; 
      font-size: 1.25rem; 
      color: #333; 
      margin-bottom: 10px;
    }
    
    .method-card p {
      font-size: 0.95rem;
      color: #555;
      margin-bottom: 10px;
      flex-grow: 1; 
      word-wrap: break-word; 
    }

    .badge {
      display: inline-block; 
      padding: 4px 10px; 
      border-radius: 6px;
      font-size: 0.75rem; 
      font-weight: bold; 
      text-transform: uppercase; 
      letter-spacing: 0.5px;
      margin-bottom: 10px; 
      color: #fff;
      white-space: nowrap;
      width: fit-content;
    }

    /* Visualizations */
    .vis-card {
      margin-bottom: 40px;
      padding: 30px;
      border-radius: 16px;
      text-align: center;
      background: #fff;
      border: 1px solid rgba(0,0,0,0.04);
      box-shadow: 0 4px 20px rgba(0,0,0,0.04);
      transition: transform 0.3s;
    }
    .vis-card:hover { transform: scale(1.005); box-shadow: 0 8px 30px rgba(0,0,0,0.08); }

    .vis-header {
      display: inline-block;
      font-size: 1.2rem;
      font-weight: 700;
      margin-bottom: 15px;
      padding: 6px 18px;
      border-radius: 50px;
    }

    /* BibTeX */
    pre {
      background-color: #2d2d2d;
      color: #ccc;
      padding: 20px;
      border-radius: 10px;
      font-size: 0.85rem;
      overflow-x: auto;
      line-height: 1.5;
      border: 1px solid #444;
    }

    @media (max-width: 768px) {
      .main-title-row { flex-direction: column; } /* Stack logo on mobile */
      .title { font-size: 2rem; }
      .teaser-img { width: 90%; }
      .container { padding: 20px 15px; }
      .method-grid { grid-template-columns: 1fr; }
    }
  </style>
  <style>
    /* --- Announcement block --- */
    .announcement {
        display: flex;
        align-items: flex-start;
        gap: 14px;
        background: #fff;
        border: 1px solid rgba(0,0,0,0.06);
        box-shadow: 0 4px 20px rgba(0,0,0,0.03);
        border-radius: 14px;
        padding: 14px 16px;
        margin: 18px auto 28px auto;
        max-width: 860px;
    }

    .announcement-bar {
        width: 6px;
        height: 26px;            /* ç«–çŸ­çº¿ */
        border-radius: 999px;
        background: var(--color-primary);
        flex: 0 0 auto;
        margin-top: 2px;
    }

    .announcement-text {
        font-size: 1.02rem;
        color: #3f3f3f;
        line-height: 1.55;
        letter-spacing: 0.1px;
    }

    .announcement-text .date {
        font-weight: 700;
        color: #333;
        margin-right: 8px;
    }

    @media (max-width: 1000px) {
        .announcement {
        margin: 14px 0 22px 0;
        padding: 12px 14px;
        }
        .announcement-text {
        font-size: 0.98rem;
        }
    }
  </style>
</head>
<body>

<div class="container">

  <div class="header-container">
    <h1 class="title">
        <div class="main-title-row">
          <img src="static/images/logo.png" alt="Logo" class="project-logo" style="height: 1.5em; width: auto;">
          <span class="title-main">CoF-T2I:</span>
        </div>
        <span class="title-sub">Video Models as Pure Visual Reasoners</span>
        <span class="title-sub">for Text-to-Image Generation</span>
    </h1>
    
    <div class="author-block">
      <span><a href="https://scholar.google.com/citations?user=dvG1nhgAAAAJ" target="_blank">Chengzhuo Tong</a><sup>*1 2</sup></span>,
      <span><a href="https://github.com/Nijikadesu/" target="_blank">Mingkun Chang</a><sup>*3</sup></span>,
      <span>Shenglong Zhang<sup>4</sup></span>,
      <span><a href="https://ryann-ran.github.io/" target="_blank">Yuran Wang</a><sup>1 2</sup></span>,
      <span>Cheng Liang<sup>2 5</sup></span>,
      <span><a href="https://zhizhengzhao.github.io/" target="_blank">Zhizheng Zhao</a><sup>1</sup></span>,
      <span><a href="https://scholar.google.com/citations?user=MHo_d3YAAAAJ" target="_blank">Bohan Zeng</a><sup>1 2</sup></span>,
      <span>Yang Shi<sup>1 2</sup></span>,
      <span><a href="https://arctanxarc.github.io/" target="_blank">Ruichuan An</a><sup>1</sup></span>,
      <span><a href="https://scholar.google.com/citations?user=Z9iB9o0AAAAJ&hl" target="_blank">Ziming Zhao</a><sup>4</sup></span>,
      <span><a href="https://guanbinli.com/" target="_blank">Guanbin Li</a><sup>3</sup></span>,
      <span>Pengfei Wan<sup>2</sup></span>,
      <span><a href="https://longo11070001.github.io/" target="_blank">Yuanxing Zhang</a><sup>2</sup></span>,
      <span><a href="https://zwt233.github.io/" target="_blank">Wentao Zhang</a><sup>â€ 1</sup></span>
    </div>


    <div class="affiliation-block">
      <div><sup>1</sup>Peking University &nbsp; <sup>2</sup>Kling Team, Kuaishou Technology &nbsp; <sup>3</sup>Sun Yat-sen University &nbsp; <sup>4</sup>Zhejiang University &nbsp; <sup>5</sup>Nanjing University</div>
      <div class="note"><sup>*</sup>Equal Contribution &nbsp; <sup>â€ </sup>Corresponding Author</div>
    </div>

    <div class="publication-links">
      <span class="link-block">
        <a href="https://arxiv.org/abs/2601.10061" target="_blank">
          <span class="icon"><i class="fas fa-file-pdf"></i></span>
          <span>Paper</span>
        </a>
      </span>
      <span class="link-block">
        <a href="https://github.com/VisionChengzhuo/CoF-T2I" target="_blank">
          <span class="icon"><i class="fab fa-github"></i></span>
          <span>Code</span>
        </a>
      </span>
      <!-- <span class="link-block">
        <a href="https://github.com/Nijikadesu" target="_blank">
          <span class="icon"><i class="fas fa-database"></i></span>
          <span>CoF-Evol-Instruct</span>
        </a>
      </span> -->
      <!-- <span class="link-block">
        <a href="https://github.com/Nijikadesu" target="_blank">
          <span class="icon"><i class="fas fa-database"></i></span>
          <span>CoF-Evol-Instruct</span>
        </a>
      </span> -->
    </div>
  </div>

 <div class="announcement">
    <div class="announcement-bar"></div>
    <div class="announcement-text">
      <span class="date">[2026.1.16]</span> ðŸš€ðŸš€ðŸš€ Paper and website live now! We are actively preparing to open-source our code, model, and dataset.
    </div>
  </div>


  <div class="section">
    <div class="image-container">
      <img src="static/images/vis_output.png" alt="Teaser Image" style="width: 100%;">
    </div>
    <div class="caption">
      <strong>Visualization of the reasoning trajectories generated by CoF-T2I.</strong> For each example, the final output is shown in <u>large</u>, and the intermediate frames are shown in <u>small</u>.
    </div>
  </div>

  <div class="section">
    <h2 class="section-title">Abstract</h2>
    <div class="content-text">
        <p>
        Recent video generation models have revealed the emergence of Chain-of-Frame (CoF) reasoning, enabling <strong>frame-by-frame visual inference</strong>. With this capability, video models have been successfully applied to various visual tasks (<i>e.g.</i>, maze solving, visual puzzles). However, their potential to enhance text-to-image (T2I) generation <i>remains largely unexplored</i> due to the absence of a <strong>clearly defined visual reasoning starting point</strong> and <strong>interpretable intermediate states</strong> in the T2I generation process.
        </p>

        <p>
        To bridge this gap, we propose <i><span class="highlight-term">CoF-T2I</span></i>, a model that <strong>integrates CoF reasoning into T2I generation</strong> via <strong>progressive visual refinement</strong>, where <i>intermediate frames act as explicit reasoning steps</i> and the <i>final frame is taken as output</i>. To establish such an explicit generation process, we curate <strong>CoF-Evol-Instruct</strong>, a dataset of CoF trajectories that models generation <i>from semantics to aesthetics</i>. To further improve quality and avoid motion artifacts, we enable <strong>independent encoding for each frame</strong>. Experiments show that CoF-T2I <strong>significantly outperforms</strong> the base video model and achieves <strong>competitive performance</strong>.
        </p>
    </div>
  </div>


  <div class="section">
    <div class="image-container">
      <img src="static/images/intro.png" alt="Intro Comparison" style="width: 45%;">
    </div>
    <div class="caption">
        <strong>Comparison of <span style="color: #333;">Inference-time Reasoning</span> Models.</strong><br>
        (a) <span style="color: #27ae60; font-weight: bold;">Exterior Reward Guidance:</span> Equipping image models with external verifier.<br>
        (b) <span style="color: #0288D1; font-weight: bold;">Interleaved Chain-of-Thought:</span> Interleaving textual planning within unified multimodal large language models.<br>
        (c) <span style="color: #E67E22; font-weight: bold;">Chain-of-Frame:</span> Our proposed video-based CoF reasoning model <strong>CoF-T2I</strong>.
    </div>
  </div>

  <div class="section">
    <h2 class="section-title">Overview</h2>
    
    <div class="image-container">
      <img src="static/images/overview.png" alt="CoF-T2I Overview" style="width: 80%;">
    </div>
    <div class="caption">
      <strong>Overview of CoF-T2I.</strong> CoF-T2I builds on a video generation backbone, reframing inference-time reasoning for T2I generation as a CoF refinement process. 
    </div>

    <div class="method-grid">
        <div class="method-card method-think">
          <span class="badge" style="background: var(--color-train); color: #6b4e2ed3">Training</span>
          <p>
            Given a CoF trajectory, we employ a video VAE to separately encode each frame, and optimize a vanilla flow-matching objective.
          </p>
        </div>
    
        <div class="method-card method-gen">
          <span class="badge" style="background: var(--color-infer); color: #2F4A6D">Inference</span>
          <p>
            Starting from noisy initialization, the model denoises to sample a progressively refined reasoning trajectory, only the final-frame latent is fully decoded and taken as the output image.
          </p>
        </div>
    
        <div class="method-card method-ref">
          <span class="badge" style="background: var(--color-quality); color: #2F5D4A">Quality Assessment</span>
          <p>
            Along the CoF trajectory, text-image alignment and aesthetic quality continue to improve.
          </p>
        </div>
      </div>
  </div>

  <div class="section">
    <h2 class="section-title">Data Curation</h2>
    
    <div class="image-container">
      <img src="static/images/pipeline.png" alt="Curation Pipeline" style="width: 80%;">
    </div>
    <div class="caption">
      <strong>Visualization of CoF-Evol-Instruct Dataset.</strong> We design a quality-aware construction pipeline to curate reasoning data, ensures both <u>sample-level diversity</u> and <u>frame-wise consistency</u>.
    </div>
  </div>

  <div class="section" id="visuals" style="background: transparent; box-shadow: none; padding: 0;">
    <h2 class="section-title">Visualizations</h2>

    <div class="vis-card">
      <div class="vis-header" style="background: #edfceb; color: #248f07;">
        <i class="fas fa-camera-retro"></i> Dataset Visualizations
      </div>
      <div class="image-container">
        <img src="static/images/vis_data.png" alt="Dataset" style="width: 80%;">
      </div>
      <div class="caption">
        <strong>Visualization of CoF-Evol-Instruct Dataset.</strong> We showcase the prompt and corresponding CoF trajectories in our data.
      </div>
    </div>
    
    <div class="vis-card">
      <div class="vis-header" style="background: #E1F5FE; color: #0288D1;">
        <i class="fas fa-exchange-alt"></i> Qualitative Comparison
      </div>
      <div class="image-container">
        <img src="static/images/vis_comp.png" alt="Comparison" style="width: 80%;">
      </div>
      <div class="caption">
        Comparison of the Wan2.1-T2V (baseline), Bagel-Think, and <strong>CoF-T2I</strong>. CoF-T2I produces satisfying results with both high photorealistic quality and precise alignment with the prompt.
      </div>
    </div>

    <div class="vis-card">
      <div class="vis-header" style="background: #FFF3E0; color: #E67E22;">
        <i class="fas fa-magic"></i> Reasoning Trajectories
      </div>
      <div class="image-container">
        <img src="static/images/vis_more.png" alt="Reflection" style="width: 80%;">
      </div>
      <div class="caption">
        <strong>Complete reasoning trajectories of CoF-T2I</strong>, including the intermediate frames and the final output alongside their corresponding prompts.
      </div>
    </div>

    <div class="vis-card">
      <div class="vis-header" style="background: #F3E5F5; color: #8E24AA;">
        <i class="fas fa-brain"></i> Step-wise Quality Evolution
      </div>
      <div class="image-container">
        <img src="static/images/evol.png" alt="Quality Evolution" style="width: 80%;">
      </div>
      <div class="caption">
        <strong>Evolution of generation quality across reasoning steps.</strong> The results exhibit a general ascending trend in performance scores across the inference steps on both GenEval (left) and Imagine-Bench (right).
      </div>
    </div>

  </div>

  <div class="section">
    <h2 class="section-title">BibTeX</h2>
    <pre><code>@misc{tong2026coft2ivideomodelspure,
      title={CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation}, 
      author={Chengzhuo Tong and Mingkun Chang and Shenglong Zhang and Yuran Wang and Cheng Liang and Zhizheng Zhao and Ruichuan An and Bohan Zeng and Yang Shi and Yifan Dai and Ziming Zhao and Guanbin Li and Pengfei Wan and Yuanxing Zhang and Wentao Zhang},
      year={2026},
      eprint={2601.10061},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2601.10061}, 
}</code></pre>
  </div>

  <footer style="text-align: center; padding: 30px 0; color: #999; font-size: 0.9rem;">
    <p>Copyright &copy; 2026 CoF-T2I Project. </p>
  </footer>

</div>

</body>
</html>